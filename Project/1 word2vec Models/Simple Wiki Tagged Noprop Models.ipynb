{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Google Drive\\\\University\\\\Dissertation\\\\Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/'+os.getlogin()+'/Google Drive/University/Dissertation'\n",
    "#datapath = 'C:/Users/'+os.getlogin()+'/Dissertation Data'\n",
    "datapath = 'E:/Dissertation Data'\n",
    "\n",
    "os.chdir(path+'/Code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April|NNP is|VBZ the|DT fourth|JJ month|NN of|IN the|DT year|NN and|CC comes|VBZ between|IN March|NNP and|CC May|NNP \n",
      "\n",
      "\n",
      "\n",
      "It|PRP is|VBZ one|CD of|IN four|CD months|NNS to|TO have|VB 30|CD days|NNS \n",
      "\n",
      "\n",
      "\n",
      "April|NNP always|RB begins|VBZ on|IN the|DT same|JJ day|NN of|IN week|NN as|IN July|NNP and|CC additionally|RB January|NNP in|IN leap|NNP years|NNS \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On Simple English wiki, POS tagged\n",
    "\n",
    "sf = open(datapath+'/Corpora/wiki/simple_20200601/Tagged/simple_20200601_tagged_clean.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for lines in range(5):\n",
    "    print(sf.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "from nltk.corpus.reader.util import read_line_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_t = PlaintextCorpusReader(datapath+'/Corpora/wiki/simple_20200601/Tagged','simple_20200601_tagged_clean.txt',\n",
    "                            word_tokenizer = WhitespaceTokenizer(),\n",
    "                            para_block_reader=read_line_block\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word and sentence generators\n",
    "\n",
    "from generators import sent_gen, word_gen, Sent_Seq, Simp_Sent_Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate n-grams\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    TrigramAssocMeasures,\n",
    "    NgramAssocMeasures,\n",
    ")\n",
    "\n",
    "from nltk.metrics.spearman import (\n",
    "    spearman_correlation,\n",
    "    ranks_from_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from corpus - 50 most frequent\n",
    "\n",
    "with open(datapath+'/Corpora/wiki/simple_20200601/Tagged/stop_clean.pkl', 'rb') as pfile:\n",
    "    stop = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0|CD',\n",
       " '1|CD',\n",
       " '2|CD',\n",
       " '3|CD',\n",
       " '4|CD',\n",
       " ':|:',\n",
       " 'A|DT',\n",
       " 'He|PRP',\n",
       " 'In|IN',\n",
       " 'It|PRP',\n",
       " 'The|DT',\n",
       " 'This|DT',\n",
       " 'also|RB',\n",
       " 'and|CC',\n",
       " 'an|DT',\n",
       " 'are|VBP',\n",
       " 'as|IN',\n",
       " 'at|IN',\n",
       " 'a|DT',\n",
       " 'be|VB',\n",
       " 'born|VBN',\n",
       " 'by|IN',\n",
       " 'can|MD',\n",
       " 'first|JJ',\n",
       " 'for|IN',\n",
       " 'from|IN',\n",
       " 'had|VBD',\n",
       " 'has|VBZ',\n",
       " 'he|PRP',\n",
       " 'his|PRP$',\n",
       " 'in|IN',\n",
       " 'is|VBZ',\n",
       " 'it|PRP',\n",
       " 'not|RB',\n",
       " 'of|IN',\n",
       " 'one|CD',\n",
       " 'on|IN',\n",
       " 'or|CC',\n",
       " 'people|NNS',\n",
       " 'that|IN',\n",
       " 'that|WDT',\n",
       " 'their|PRP$',\n",
       " 'they|PRP',\n",
       " 'the|DT',\n",
       " 'to|IN',\n",
       " 'to|TO',\n",
       " 'was|VBD',\n",
       " 'were|VBD',\n",
       " 'which|WDT',\n",
       " 'with|IN'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 20\n",
    "eval_count = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(waystations|NNS, shuku|NN-eki|NN)</td>\n",
       "      <td>20</td>\n",
       "      <td>-607.368914</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(conventionally|RB, delimit|VBP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-638.137312</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(mutually|RB, intelligible|JJ)</td>\n",
       "      <td>20</td>\n",
       "      <td>-641.588322</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ottava|NN, rima|NN)</td>\n",
       "      <td>21</td>\n",
       "      <td>-649.952795</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(spiny|JJ, dogfish|NN)</td>\n",
       "      <td>20</td>\n",
       "      <td>-655.280617</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(He|PRP, wrote|VBD, about|IN)</td>\n",
       "      <td>67</td>\n",
       "      <td>-4314.528245</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(for|IN, every|DT, 1000|CD)</td>\n",
       "      <td>72</td>\n",
       "      <td>-4314.701644</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(a|DT, combined|VBN)</td>\n",
       "      <td>89</td>\n",
       "      <td>-4314.758077</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(on|IN, July|NNP, 28|CD)</td>\n",
       "      <td>66</td>\n",
       "      <td>-4315.075053</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(of|IN, Armenian|JJ)</td>\n",
       "      <td>87</td>\n",
       "      <td>-4315.243283</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ngram  freq      poisson  len  batch\n",
       "0       (waystations|NNS, shuku|NN-eki|NN)    20  -607.368914    2      1\n",
       "1         (conventionally|RB, delimit|VBP)    20  -638.137312    2      1\n",
       "2           (mutually|RB, intelligible|JJ)    20  -641.588322    2      1\n",
       "3                     (ottava|NN, rima|NN)    21  -649.952795    2      1\n",
       "4                   (spiny|JJ, dogfish|NN)    20  -655.280617    2      1\n",
       "...                                    ...   ...          ...  ...    ...\n",
       "149995       (He|PRP, wrote|VBD, about|IN)    67 -4314.528245    3      3\n",
       "149996         (for|IN, every|DT, 1000|CD)    72 -4314.701644    3      8\n",
       "149997                (a|DT, combined|VBN)    89 -4314.758077    2      1\n",
       "149998            (on|IN, July|NNP, 28|CD)    66 -4315.075053    3      1\n",
       "149999                (of|IN, Armenian|JJ)    87 -4315.243283    2      3\n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval = pd.read_pickle(datapath+'/Corpora/wiki/simple_20200601/Tagged/ngram_eval_nopn.pkl')\n",
    "\n",
    "ngram_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = max(ngram_eval.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1     45750\n",
       " 2     35000\n",
       " 3     21918\n",
       " 4     16889\n",
       " 5      9401\n",
       " 6      7370\n",
       " 7      4178\n",
       " 8      4003\n",
       " 9      2009\n",
       "-2      1001\n",
       " 10      827\n",
       " 11      575\n",
       " 12      424\n",
       "-1       369\n",
       " 13      179\n",
       " 14       77\n",
       " 15       30\n",
       "Name: batch, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Flatten down to a single number\n",
    "def cosim(x,y):\n",
    "    return cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 2 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 3 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 4 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 5 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 6 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 7 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 8 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 9 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 10 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 11 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 12 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 13 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 14 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 15 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Wall time: 2h 57min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "for bb in range(batch_count):\n",
    "    print('Processing batch {} of {}'.format(bb+1,batch_count))\n",
    "    \n",
    "    # Subset DataFrame\n",
    "    batch_dfs[bb] = ngram_eval[ngram_eval.batch == bb+1].reset_index(drop=True)\n",
    "    \n",
    "    # Initialise MWETokenizer\n",
    "    batch_token_mwe = MWETokenizer(list(batch_dfs[bb].ngram) , separator='+')\n",
    "    \n",
    "    # Build model\n",
    "    print('Building word2vec model')\n",
    "    sents_mwe = Simp_Sent_Seq(simp_t, tokenizer = batch_token_mwe)\n",
    "    \n",
    "    print(' Training model')\n",
    "    batch_model = Word2Vec(sents_mwe,\n",
    "                             min_count = 20,  # 20 matches R&E on EN Wiki\n",
    "                             size = 400,\n",
    "                             workers = 8,\n",
    "                             window = 5,\n",
    "                             sg = 0,         # CBOW\n",
    "                             sample = 10e-5, # Subsampling\n",
    "                             negative = 10\n",
    "                            )\n",
    "\n",
    "    # Save model\n",
    "    print(' Saving model')\n",
    "    batch_model.save(datapath+'/Models/1 w2v/Tagged/simp_tagged_nopn_batch{}.model'.format(bb+1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
