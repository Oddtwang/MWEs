{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Google Drive\\\\University\\\\Dissertation\\\\Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/'+os.getlogin()+'/Google Drive/University/Dissertation'\n",
    "datapath = 'E:/Dissertation Data'\n",
    "#datapath = 'C:/Users/'+os.getlogin()+'/Dissertation Data'\n",
    "\n",
    "os.chdir(path+'/Code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "#from glove import Corpus, Glove\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    TrigramAssocMeasures,\n",
    "    NgramAssocMeasures,\n",
    ")\n",
    "\n",
    "from nltk.metrics.spearman import (\n",
    "    spearman_correlation,\n",
    "    ranks_from_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then automatically score the million most strongly associated\n",
    "n-grams (i.e., roughly the top 5% of the Poisson-ranked\n",
    "list) for compositionality.\n",
    "\n",
    "Using word2vec (Mikolov et al., 2013) with the parameters\n",
    "found to be most effective by Baroni et al. (2014), we\n",
    "build a word embedding vector for every simplex word in\n",
    "the vocabulary (ca. 1M types), as well as for each MWE candidate.\n",
    "\n",
    "* Continuous bag of words model with 400-dimensional vectors, window size 5, subsampling with t = 10^-5, negative sampling with 10 samples. We build vectors only for tokens observed 20 times or more in the corpus.\n",
    "\n",
    "We then compute the cosine similarity of the vector\n",
    "representation for a MWE candidate with the vectors of its\n",
    "constituent words, and take the arithmetic mean. \n",
    "In scoring\n",
    "the compositionality of a candidate, we do not measure the\n",
    "cosine similarity of the MWE with any stop words it may\n",
    "contain, as stop words may be assumed to be semantically\n",
    "uninformative.\n",
    "* Stop words are taken here to be the 50 most frequent words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from corpus - 50 most frequent\n",
    "\n",
    "with open(datapath+'/Corpora/wiki/simple_20200601/Tagged/stop_clean.pkl', 'rb') as pfile:\n",
    "    stop = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0|CD',\n",
       " '1|CD',\n",
       " '2|CD',\n",
       " '3|CD',\n",
       " '4|CD',\n",
       " ':|:',\n",
       " 'A|DT',\n",
       " 'He|PRP',\n",
       " 'In|IN',\n",
       " 'It|PRP',\n",
       " 'The|DT',\n",
       " 'This|DT',\n",
       " 'also|RB',\n",
       " 'and|CC',\n",
       " 'an|DT',\n",
       " 'are|VBP',\n",
       " 'as|IN',\n",
       " 'at|IN',\n",
       " 'a|DT',\n",
       " 'be|VB',\n",
       " 'born|VBN',\n",
       " 'by|IN',\n",
       " 'can|MD',\n",
       " 'first|JJ',\n",
       " 'for|IN',\n",
       " 'from|IN',\n",
       " 'had|VBD',\n",
       " 'has|VBZ',\n",
       " 'he|PRP',\n",
       " 'his|PRP$',\n",
       " 'in|IN',\n",
       " 'is|VBZ',\n",
       " 'it|PRP',\n",
       " 'not|RB',\n",
       " 'of|IN',\n",
       " 'one|CD',\n",
       " 'on|IN',\n",
       " 'or|CC',\n",
       " 'people|NNS',\n",
       " 'that|IN',\n",
       " 'that|WDT',\n",
       " 'their|PRP$',\n",
       " 'they|PRP',\n",
       " 'the|DT',\n",
       " 'to|IN',\n",
       " 'to|TO',\n",
       " 'was|VBD',\n",
       " 'were|VBD',\n",
       " 'which|WDT',\n",
       " 'with|IN'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Grădina|NNP, Zoologică|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Ving|NNP, Rhames|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Karlovy|NNP, Vary|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Gharb|NNP-Chrarda|NNP-Beni|NNP, Hssen|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(waystations|NNS, shuku|NN-eki|NN)</td>\n",
       "      <td>20</td>\n",
       "      <td>-607.368914</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(supporters|NNS, of|IN, the|DT)</td>\n",
       "      <td>47</td>\n",
       "      <td>-3262.258611</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(episodes|NNS, and|CC)</td>\n",
       "      <td>64</td>\n",
       "      <td>-3262.274573</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(as|IN, old|JJ, as|IN)</td>\n",
       "      <td>48</td>\n",
       "      <td>-3262.394255</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(John|NNP, Kerry|NNP)</td>\n",
       "      <td>77</td>\n",
       "      <td>-3262.495991</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(is|VBZ, a|DT, bit|NN)</td>\n",
       "      <td>48</td>\n",
       "      <td>-3262.496741</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ngram  freq      poisson  len  \\\n",
       "0                      (Grădina|NNP, Zoologică|NNP)    20  -605.961128    2   \n",
       "1                            (Ving|NNP, Rhames|NNP)    20  -605.961128    2   \n",
       "2                           (Karlovy|NNP, Vary|NNP)    20  -605.961128    2   \n",
       "3       (Gharb|NNP-Chrarda|NNP-Beni|NNP, Hssen|NNP)    20  -605.961128    2   \n",
       "4                (waystations|NNS, shuku|NN-eki|NN)    20  -607.368914    2   \n",
       "...                                             ...   ...          ...  ...   \n",
       "149995              (supporters|NNS, of|IN, the|DT)    47 -3262.258611    3   \n",
       "149996                       (episodes|NNS, and|CC)    64 -3262.274573    2   \n",
       "149997                       (as|IN, old|JJ, as|IN)    48 -3262.394255    3   \n",
       "149998                        (John|NNP, Kerry|NNP)    77 -3262.495991    2   \n",
       "149999                       (is|VBZ, a|DT, bit|NN)    48 -3262.496741    3   \n",
       "\n",
       "        batch  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "149995      4  \n",
       "149996      3  \n",
       "149997     -1  \n",
       "149998      1  \n",
       "149999      1  \n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval = pd.read_pickle(datapath+'/Corpora/wiki/simple_20200601/Tagged/ngram_eval_clean.pkl')\n",
    "\n",
    "ngram_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 20\n",
    "eval_count = 150000\n",
    "\n",
    "batch_count = max(ngram_eval.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1     47463\n",
       " 2     33035\n",
       " 3     24270\n",
       " 4     16305\n",
       " 5     10058\n",
       " 6      7289\n",
       " 7      4847\n",
       " 8      2491\n",
       " 9      1465\n",
       " 10      942\n",
       "-2       814\n",
       " 11      457\n",
       "-1       186\n",
       " 12      171\n",
       " 13      118\n",
       " 14       65\n",
       " 15       24\n",
       "Name: batch, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Flatten down to a single number\n",
    "def cosim(x,y):\n",
    "    return cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwe_score(exp, model, stats_frame):\n",
    "    \n",
    "    # Combined token for MWE\n",
    "    mwetoken = '+'.join(exp)\n",
    "\n",
    "    # Stopwords - 1 if component is a stopword, 0 if present, -1 if simplex word missing from vocab, -2 if MWE missing\n",
    "    sws = []\n",
    "    # Component vectors\n",
    "    cvs = []\n",
    "\n",
    "    #  Neighbours in original & MWE-aware space\n",
    "    #oldn = []\n",
    "    #newn = []\n",
    "\n",
    "    # List of individual word similarities (where present in the vocab)\n",
    "    css = []\n",
    "\n",
    "    # Empty array\n",
    "    earr = np.empty(400)\n",
    "    earr[:] = np.nan\n",
    "\n",
    "    # Check that combined token exists in the vocab. This protects against inflation of n-gram counts caused by repeats\n",
    "    #  of the same token (e.g. in lists like https://simple.wikipedia.org/wiki/List_of_cities,_towns_and_villages_in_Fars_Province)\n",
    "    if mwetoken in batch_model.wv.vocab:\n",
    "\n",
    "        mwv = model.wv[mwetoken]\n",
    "\n",
    "        for w in exp:\n",
    "            if w in model.wv.vocab:\n",
    "                cvs.append(model.wv[w])\n",
    "\n",
    "                #oldn.append(batch_model.wv.most_similar(w, topn=5))\n",
    "\n",
    "                if w in stop:\n",
    "                    sws.append(1)\n",
    "                    css.append(np.nan)\n",
    "                else:\n",
    "                    sws.append(0)\n",
    "                    css.append(cosim(model.wv[w], mwv ))\n",
    "\n",
    "            # If component is absent from vocab\n",
    "            else:\n",
    "                sws.append(-1)\n",
    "                cvs.append(earr)\n",
    "                css.append(np.nan)\n",
    "\n",
    "                #oldn.append([])\n",
    "\n",
    "        #  Mean cosim\n",
    "        if min(sws) >= 0:\n",
    "            cs = np.nanmean(css)\n",
    "        else:\n",
    "            cs = np.nan\n",
    "\n",
    "        #newn = batch_model.wv.most_similar(mwetoken, topn=5)\n",
    "\n",
    "    # Combined token missing from vocab - mark with defaults\n",
    "    else:\n",
    "        sws = [-2]\n",
    "        mwv = np.empty(400)\n",
    "        mwv[:] = np.nan\n",
    "        cs = np.nan\n",
    "\n",
    "\n",
    "    # Append to stats df\n",
    "    return stats_frame.append({\n",
    "        'ngram'  : exp,\n",
    "        'stopwords' : sws,\n",
    "        'mwe_vector' : mwv,\n",
    "        'component_vectors' : cvs,\n",
    "        'component_cosims'  : css,\n",
    "        'cosine_sim'  : cs,\n",
    "        #'base_nearest': oldn,\n",
    "        #'mwe_nearest' : newn,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/47463: Grădina|NNP+Zoologică|NNP\n",
      " MWE 5000/47463: Dutch|NNP+actress|NN\n",
      " MWE 10000/47463: front|JJ+teeth|NNS\n",
      " MWE 15000/47463: 2006/07|CD+Segunda|NNP\n",
      " MWE 20000/47463: both|DT+directions|NNS\n",
      " MWE 25000/47463: East|NNP+End|NN\n",
      " MWE 30000/47463: a|DT+living|VBG+thing|NN\n",
      " MWE 35000/47463: as|IN+commander|NN\n",
      " MWE 40000/47463: the|DT+invading|VBG\n",
      " MWE 45000/47463: Edward|NNP+IV|NNP\n",
      "Processing batch 2 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/33035: Senateacting|NNP+asGovernor|NNP\n",
      " MWE 5000/33035: Energy|NNP+is|VBZ\n",
      " MWE 10000/33035: their|PRP$+computers|NNS\n",
      " MWE 15000/33035: gas|NN+was|VBD\n",
      " MWE 20000/33035: it|PRP+means|VBZ+to|TO\n",
      " MWE 25000/33035: Age|NN+of|IN+the|DT\n",
      " MWE 30000/33035: theory|NN+is|VBZ+a|DT\n",
      "Processing batch 3 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/24270: Ilhwa|NNP+Chunma|NNP\n",
      " MWE 5000/24270: Have|VBP+You|PRP+Ever|RB\n",
      " MWE 10000/24270: the|DT+Red|JJ\n",
      " MWE 15000/24270: between|IN+1980|CD+and|CC\n",
      " MWE 20000/24270: In|IN+those|DT+days|NNS\n",
      "Processing batch 4 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/16305: Kuch|NNP+Hota|NNP+Hai|NNP\n",
      " MWE 1000/16305: heavy|JJ+machine|NN\n",
      " MWE 2000/16305: Premier|NNP+League|NNP+5th|NN\n",
      " MWE 3000/16305: her|PRP$+candidacy|NN\n",
      " MWE 4000/16305: agreed|VBD+to|TO+take|VB\n",
      " MWE 5000/16305: Baptist|NNP+Convention|NNP\n",
      " MWE 6000/16305: Lyonnais|NNP+Ligue|NNP+1|CD\n",
      " MWE 7000/16305: Southern|NNP+Methodist|NNP+University|NNP\n",
      " MWE 8000/16305: by|IN+Beyoncé|NNP\n",
      " MWE 9000/16305: and|CC+sleep|NN\n",
      " MWE 10000/16305: expressed|VBN+as|IN+a|DT\n",
      " MWE 11000/16305: 2005|CD+JEF|NNP+United|NNP\n",
      " MWE 12000/16305: English|JJ+German|JJ\n",
      " MWE 13000/16305: player|NN+at|IN\n",
      " MWE 14000/16305: make|VB+it|PRP+a|DT\n",
      " MWE 15000/16305: to|IN+higher|JJR\n",
      " MWE 16000/16305: positive|JJ+or|CC\n",
      "Processing batch 5 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/10058: d’Art|NNP+Moderne|NNP+de|FW\n",
      " MWE 1000/10058: of|IN+Ice|NN\n",
      " MWE 2000/10058: the|DT+Australian|JJ+Senate|NNP\n",
      " MWE 3000/10058: had|VBD+agreed|VBN+to|TO\n",
      " MWE 4000/10058: June|NNP+1936|CD\n",
      " MWE 5000/10058: part|NN+for|IN\n",
      " MWE 6000/10058: can|MD+be|VB+identified|VBN\n",
      " MWE 7000/10058: they|PRP+were|VBD+very|RB\n",
      " MWE 8000/10058: name|NN+now|RB+known|VBN\n",
      " MWE 9000/10058: is|VBZ+distributed|VBN\n",
      " MWE 10000/10058: February|NNP+1998|CD\n",
      "Processing batch 6 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/7289: de|FW+mécanique|FW+et|FW\n",
      " MWE 1000/7289: tournament|NN+at|IN+the|DT\n",
      " MWE 2000/7289: numbers|NNS+on|IN+the|DT\n",
      " MWE 3000/7289: for|IN+religious|JJ+reasons|NNS\n",
      " MWE 4000/7289: that|WDT+caused|VBD+the|DT\n",
      " MWE 5000/7289: chart|NN+positions|NNS+Sales|NNS\n",
      " MWE 6000/7289: town|NN+and|CC+the|DT\n",
      " MWE 7000/7289: Hitler|NNP+and|CC\n",
      "Processing batch 7 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/4847: et|FW+hélicoptères|FW+militaires|FW\n",
      " MWE 200/4847: bar|NN+:|:+1984|CD\n",
      " MWE 400/4847: to|IN+the|DT+parliament|NN\n",
      " MWE 600/4847: the|DT+figure|NN+of|IN\n",
      " MWE 800/4847: may|MD+be|VB+of|IN\n",
      " MWE 1000/4847: Country|NNP+Party|NNP\n",
      " MWE 1200/4847: than|IN+90%|CD+of|IN\n",
      " MWE 1400/4847: a|DT+Miniseries|NNP+or|CC\n",
      " MWE 1600/4847: The|DT+album|NN's+first|JJ\n",
      " MWE 1800/4847: him|PRP+as|IN+one|CD\n",
      " MWE 2000/4847: Fresno|NNP+California|NNP\n",
      " MWE 2200/4847: and|CC+a|DT+dark|JJ\n",
      " MWE 2400/4847: the|DT+Union|NNP+lines|NNS\n",
      " MWE 2600/4847: 1990|CD+until|IN\n",
      " MWE 2800/4847: the|DT+World|NNP+of|IN\n",
      " MWE 3000/4847: a|DT+vision|NN+of|IN\n",
      " MWE 3200/4847: former|JJ+Dutch|JJ\n",
      " MWE 3400/4847: the|DT+Year|NNP+2010|CD\n",
      " MWE 3600/4847: the|DT+atmosphere|NN+of|IN\n",
      " MWE 3800/4847: the|DT+understanding|NN+of|IN\n",
      " MWE 4000/4847: where|WRB+the|DT+person|NN\n",
      " MWE 4200/4847: Gulf|NN+of|IN+Mexico|NNP\n",
      " MWE 4400/4847: and|CC+anime|NN\n",
      " MWE 4600/4847: are|VBP+put|VBN+together|RB\n",
      " MWE 4800/4847: Penalties|NNS+in|IN+minutes|NNS\n",
      "Processing batch 8 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/2491: was|VBD+French|JJ\n",
      " MWE 200/2491: The|DT+ongoing|JJ+COVID|NNP\n",
      " MWE 400/2491: born|VBN+July|NNP+21|CD\n",
      " MWE 600/2491: of|IN+New|NNP+France|NNP\n",
      " MWE 800/2491: been|VBN+removed|VBN+from|IN\n",
      " MWE 1000/2491: an|DT+Olympic|JJ+gold|NN\n",
      " MWE 1200/2491: there|EX+is|VBZ+too|RB\n",
      " MWE 1400/2491: sources|NNS+such|JJ+as|IN\n",
      " MWE 1600/2491: joined|VBD+the|DT+Royal|NNP\n",
      " MWE 1800/2491: clear|JJ+and|CC\n",
      " MWE 2000/2491: by|IN+some|DT+to|TO\n",
      " MWE 2200/2491: school|NN+is|VBZ+in|IN\n",
      " MWE 2400/2491: the|DT+northern|JJ+end|NN\n",
      "Processing batch 9 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/1465: French|JJ+the|DT\n",
      " MWE 200/1465: to|IN+Southern|NNP\n",
      " MWE 400/1465: that|IN+the|DT+US|NNP\n",
      " MWE 600/1465: 2005|CD+14|CD+December|NNP\n",
      " MWE 800/1465: that|IN+the|DT+Soviet|NNP\n",
      " MWE 1000/1465: —|:+January|NNP+3|CD\n",
      " MWE 1200/1465: from|IN+acting|VBG\n",
      " MWE 1400/1465: from|IN+the|DT+planet|NN\n",
      "Processing batch 10 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/942: New|NNP+Zealand|NNP+d|NNP\n",
      " MWE 200/942: for|IN+a|DT+computer|NN\n",
      " MWE 400/942: by|IN+the|DT+way|NN\n",
      " MWE 600/942: the|DT+first|JJ+women|NNS\n",
      " MWE 800/942: have|VB+the|DT+right|JJ\n",
      "Processing batch 11 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/457: to|TO+pay|VB+to|TO\n",
      " MWE 200/457: to|IN+1995|CD+and|CC\n",
      " MWE 400/457: I|PRP+Call|VBP+Music|NNP\n",
      "Processing batch 12 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/171: in|IN+Singapore|NNP+and|CC\n",
      "Processing batch 13 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/118: a|DT+site|NN+of|IN\n",
      "Processing batch 14 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/65: in|IN+Munich|NNP+in|IN\n",
      "Processing batch 15 of 15\n",
      "Loading w2v model\n",
      "Gathering MWE stats\n",
      " MWE 0/24: in|IN+2006|CD+in|IN\n",
      "Wall time: 11min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "for bb in range(batch_count):\n",
    "    print('Processing batch {} of {}'.format(bb+1,batch_count))\n",
    "    \n",
    "    # Subset DataFrame\n",
    "    batch_dfs[bb] = ngram_eval[ngram_eval.batch == bb+1].reset_index(drop=True)\n",
    "    \n",
    "    # Initialise MWETokenizer\n",
    "    batch_token_mwe = MWETokenizer(list(batch_dfs[bb].ngram) , separator='+')\n",
    "    \n",
    "    # Load model\n",
    "    print('Loading w2v model')\n",
    "\n",
    "    batch_model = Word2Vec.load(datapath+'/Models/1 w2v/Tagged/simp_tagged_clean_batch{}.model'.format(bb+1))\n",
    "    \n",
    "    print('Gathering MWE stats')\n",
    "    \n",
    "    # For each MWE, evaluate stats. Record vectors (in case we want to calculate different metrics later).\n",
    "    statsf = pd.DataFrame(columns=['ngram', 'stopwords', 'mwe_vector', 'component_vectors', 'component_cosims', \n",
    "                                   'cosine_sim']) # , 'base_nearest', 'mwe_nearest'\n",
    "    batch_len = len(batch_dfs[bb].ngram)\n",
    "    if batch_len >= 20000: \n",
    "        printer = 5000\n",
    "    elif batch_len >= 5000: \n",
    "        printer = 1000\n",
    "    else:\n",
    "        printer = 200\n",
    "        \n",
    "    _i = 0\n",
    "    \n",
    "    for exp in batch_dfs[bb].ngram:\n",
    "        if _i % printer == 0:\n",
    "            print(' MWE '+str(_i)+'/'+str(batch_len)+': '+'+'.join(exp))\n",
    "        _i += 1\n",
    "        \n",
    "        statsf = mwe_score(exp, batch_model, statsf)\n",
    "        \n",
    "    #  Join back onto DataFrame\n",
    "    batch_dfs[bb] = batch_dfs[bb].merge(statsf, on='ngram')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending batch 1 of 15\n",
      "Appending batch 2 of 15\n",
      "Appending batch 3 of 15\n",
      "Appending batch 4 of 15\n",
      "Appending batch 5 of 15\n",
      "Appending batch 6 of 15\n",
      "Appending batch 7 of 15\n",
      "Appending batch 8 of 15\n",
      "Appending batch 9 of 15\n",
      "Appending batch 10 of 15\n",
      "Appending batch 11 of 15\n",
      "Appending batch 12 of 15\n",
      "Appending batch 13 of 15\n",
      "Appending batch 14 of 15\n",
      "Appending batch 15 of 15\n"
     ]
    }
   ],
   "source": [
    "# Merge dataframes, sort by compositionality metric, export\n",
    "\n",
    "# Also want the default batches with batch no < 0\n",
    "all_batches = ngram_eval[ngram_eval.batch < 0]\n",
    "\n",
    "for d in range(batch_count):\n",
    "    print('Appending batch {} of '.format(d+1)+str(batch_count))\n",
    "    all_batches = all_batches.append(batch_dfs[d])\n",
    "    \n",
    "all_batches = all_batches.sort_values('cosine_sim')\n",
    "all_batches = all_batches.reset_index(drop=True)\n",
    "\n",
    "all_batches.to_csv(datapath+'/Models/1 w2v/Tagged/Results/simp_tagged_clean_output_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>mwe_vector</th>\n",
       "      <th>component_vectors</th>\n",
       "      <th>component_cosims</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(was|VBD, born|VBN, here|RB)</td>\n",
       "      <td>32</td>\n",
       "      <td>-2115.954533</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>[0.13005322, -0.0035864385, 0.06179243, 0.0041...</td>\n",
       "      <td>[[-0.5004486, -0.5991892, -0.7888268, 0.548910...</td>\n",
       "      <td>[nan, nan, -0.26227063]</td>\n",
       "      <td>-0.262271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(to|IN, found|VBN)</td>\n",
       "      <td>54</td>\n",
       "      <td>-2767.761057</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[-0.12694843, -0.36336574, 0.006913207, -0.103...</td>\n",
       "      <td>[[-1.3017458, -0.18562949, 0.7762788, -0.00783...</td>\n",
       "      <td>[nan, -0.16069299]</td>\n",
       "      <td>-0.160693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(:|:, end|VB)</td>\n",
       "      <td>23</td>\n",
       "      <td>-1078.890289</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[-0.004778812, -0.07245265, -0.030748673, 0.13...</td>\n",
       "      <td>[[-0.29579192, -0.5646692, 0.25214067, 0.59772...</td>\n",
       "      <td>[nan, -0.15938467]</td>\n",
       "      <td>-0.159385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(born|VBN, in|IN, then|RB)</td>\n",
       "      <td>26</td>\n",
       "      <td>-1826.089851</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>[0.12961091, -0.11182376, 0.025480177, 0.12456...</td>\n",
       "      <td>[[1.6495386, 0.88697046, -0.10751744, -0.18025...</td>\n",
       "      <td>[nan, nan, -0.15382023]</td>\n",
       "      <td>-0.153820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(is|VBZ, both|CC, the|DT)</td>\n",
       "      <td>35</td>\n",
       "      <td>-2521.230138</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>[0.07370354, -0.02085001, 0.0160751, 0.1190704...</td>\n",
       "      <td>[[-0.063559085, -0.33317208, -0.14216833, 1.21...</td>\n",
       "      <td>[nan, -0.14895083, nan]</td>\n",
       "      <td>-0.148951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(of|IN, Chaffin|NNP's, Farm|NNP)</td>\n",
       "      <td>58</td>\n",
       "      <td>-3167.096834</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, -1, 0]</td>\n",
       "      <td>[0.08349732, 0.009498704, -0.1335339, 0.100189...</td>\n",
       "      <td>[[-0.45105252, 0.16672108, 0.04918785, -0.3164...</td>\n",
       "      <td>[nan, nan, 0.23580538]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(NHL|NNP, Plus|NNP/Minus|NNP, Award|NNP)</td>\n",
       "      <td>38</td>\n",
       "      <td>-1973.585423</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[0, -1, 0]</td>\n",
       "      <td>[0.2911135, -0.031326327, -0.11115643, 0.06510...</td>\n",
       "      <td>[[1.0337293, 1.1556895, -0.19845621, 2.1994393...</td>\n",
       "      <td>[0.14594129, nan, 0.28706568]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(the|DT, Goblet|NNP, of|IN)</td>\n",
       "      <td>40</td>\n",
       "      <td>-2656.494897</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, -1, 1]</td>\n",
       "      <td>[0.32959798, -0.095800444, -0.000788789, 0.312...</td>\n",
       "      <td>[[-0.6403279, 0.068406515, -0.37032217, -0.525...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(state|NN, state|NN, state|NN)</td>\n",
       "      <td>48</td>\n",
       "      <td>-2965.054834</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[-2]</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(Cup|NNP, Winners|NNPS', Cup|NNP)</td>\n",
       "      <td>46</td>\n",
       "      <td>-2373.843488</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>[0, -1, 0]</td>\n",
       "      <td>[0.21187826, -0.26578996, -0.27447316, 0.15538...</td>\n",
       "      <td>[[1.563649, 1.0271813, -0.6749807, -0.67830414...</td>\n",
       "      <td>[0.5904629, nan, 0.5904629]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ngram  freq      poisson  len  \\\n",
       "0                   (was|VBD, born|VBN, here|RB)    32 -2115.954533    3   \n",
       "1                             (to|IN, found|VBN)    54 -2767.761057    2   \n",
       "2                                  (:|:, end|VB)    23 -1078.890289    2   \n",
       "3                     (born|VBN, in|IN, then|RB)    26 -1826.089851    3   \n",
       "4                      (is|VBZ, both|CC, the|DT)    35 -2521.230138    3   \n",
       "...                                          ...   ...          ...  ...   \n",
       "149995          (of|IN, Chaffin|NNP's, Farm|NNP)    58 -3167.096834    3   \n",
       "149996  (NHL|NNP, Plus|NNP/Minus|NNP, Award|NNP)    38 -1973.585423    3   \n",
       "149997               (the|DT, Goblet|NNP, of|IN)    40 -2656.494897    3   \n",
       "149998            (state|NN, state|NN, state|NN)    48 -2965.054834    3   \n",
       "149999         (Cup|NNP, Winners|NNPS', Cup|NNP)    46 -2373.843488    3   \n",
       "\n",
       "        batch   stopwords                                         mwe_vector  \\\n",
       "0           7   [1, 1, 0]  [0.13005322, -0.0035864385, 0.06179243, 0.0041...   \n",
       "1           7      [1, 0]  [-0.12694843, -0.36336574, 0.006913207, -0.103...   \n",
       "2           2      [1, 0]  [-0.004778812, -0.07245265, -0.030748673, 0.13...   \n",
       "3           3   [1, 1, 0]  [0.12961091, -0.11182376, 0.025480177, 0.12456...   \n",
       "4          11   [1, 0, 1]  [0.07370354, -0.02085001, 0.0160751, 0.1190704...   \n",
       "...       ...         ...                                                ...   \n",
       "149995      9  [1, -1, 0]  [0.08349732, 0.009498704, -0.1335339, 0.100189...   \n",
       "149996     10  [0, -1, 0]  [0.2911135, -0.031326327, -0.11115643, 0.06510...   \n",
       "149997     10  [1, -1, 1]  [0.32959798, -0.095800444, -0.000788789, 0.312...   \n",
       "149998     10        [-2]  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "149999     12  [0, -1, 0]  [0.21187826, -0.26578996, -0.27447316, 0.15538...   \n",
       "\n",
       "                                        component_vectors  \\\n",
       "0       [[-0.5004486, -0.5991892, -0.7888268, 0.548910...   \n",
       "1       [[-1.3017458, -0.18562949, 0.7762788, -0.00783...   \n",
       "2       [[-0.29579192, -0.5646692, 0.25214067, 0.59772...   \n",
       "3       [[1.6495386, 0.88697046, -0.10751744, -0.18025...   \n",
       "4       [[-0.063559085, -0.33317208, -0.14216833, 1.21...   \n",
       "...                                                   ...   \n",
       "149995  [[-0.45105252, 0.16672108, 0.04918785, -0.3164...   \n",
       "149996  [[1.0337293, 1.1556895, -0.19845621, 2.1994393...   \n",
       "149997  [[-0.6403279, 0.068406515, -0.37032217, -0.525...   \n",
       "149998                                                 []   \n",
       "149999  [[1.563649, 1.0271813, -0.6749807, -0.67830414...   \n",
       "\n",
       "                     component_cosims  cosine_sim  \n",
       "0             [nan, nan, -0.26227063]   -0.262271  \n",
       "1                  [nan, -0.16069299]   -0.160693  \n",
       "2                  [nan, -0.15938467]   -0.159385  \n",
       "3             [nan, nan, -0.15382023]   -0.153820  \n",
       "4             [nan, -0.14895083, nan]   -0.148951  \n",
       "...                               ...         ...  \n",
       "149995         [nan, nan, 0.23580538]         NaN  \n",
       "149996  [0.14594129, nan, 0.28706568]         NaN  \n",
       "149997                [nan, nan, nan]         NaN  \n",
       "149998                             []         NaN  \n",
       "149999    [0.5904629, nan, 0.5904629]         NaN  \n",
       "\n",
       "[150000 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(all_batches.stopwords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeWarning: Mean of empty slice\n",
    "#  Appear to be taking the np.nanmean() of a list of NaNs\n",
    "\n",
    "def inval(inlist,val=0):\n",
    "    if type(inlist) != list:\n",
    "        return False\n",
    "    return val in inlist\n",
    "\n",
    "#nozero = all_batches[~all_batches.stopwords.apply(inval)]\n",
    "\n",
    "#minone = nozero[nozero.stopwords.apply(inval,val=-1)]\n",
    "\n",
    "#minone[minone.stopwords.apply(inval,val=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_res_light = all_batches.drop(columns=['mwe_vector', 'component_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_res_light.to_csv(datapath+'/Models/1 w2v/Tagged/Results/simp_tagged_clean_light_001.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
