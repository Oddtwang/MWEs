{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Google Drive\\\\University\\\\Dissertation\\\\Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/'+os.getlogin()+'/Google Drive/University/Dissertation'\n",
    "#datapath = 'C:/Users/'+os.getlogin()+'/Dissertation Data'\n",
    "datapath = 'E:/Dissertation Data'\n",
    "\n",
    "os.chdir(path+'/Code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April|NNP is|VBZ the|DT fourth|JJ month|NN of|IN the|DT year|NN and|CC comes|VBZ between|IN March|NNP and|CC May|NNP \n",
      "\n",
      "\n",
      "\n",
      "It|PRP is|VBZ one|CD of|IN four|CD months|NNS to|TO have|VB 30|CD days|NNS \n",
      "\n",
      "\n",
      "\n",
      "April|NNP always|RB begins|VBZ on|IN the|DT same|JJ day|NN of|IN week|NN as|IN July|NNP and|CC additionally|RB January|NNP in|IN leap|NNP years|NNS \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On Simple English wiki, POS tagged\n",
    "\n",
    "sf = open(datapath+'/Corpora/wiki/simple_20200601/Tagged/simple_20200601_tagged_clean.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for lines in range(5):\n",
    "    print(sf.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "from nltk.corpus.reader.util import read_line_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_t = PlaintextCorpusReader(datapath+'/Corpora/wiki/simple_20200601/Tagged','simple_20200601_tagged_clean.txt',\n",
    "                            word_tokenizer = WhitespaceTokenizer(),\n",
    "                            para_block_reader=read_line_block\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word and sentence generators\n",
    "\n",
    "from generators import sent_gen, word_gen, Sent_Seq, Simp_Sent_Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate n-grams\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    TrigramAssocMeasures,\n",
    "    NgramAssocMeasures,\n",
    ")\n",
    "\n",
    "from nltk.metrics.spearman import (\n",
    "    spearman_correlation,\n",
    "    ranks_from_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from corpus - 50 most frequent\n",
    "\n",
    "with open(datapath+'/Corpora/wiki/simple_20200601/Tagged/stop_clean.pkl', 'rb') as pfile:\n",
    "    stop = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0|CD',\n",
       " '1|CD',\n",
       " '2|CD',\n",
       " '3|CD',\n",
       " '4|CD',\n",
       " ':|:',\n",
       " 'A|DT',\n",
       " 'He|PRP',\n",
       " 'In|IN',\n",
       " 'It|PRP',\n",
       " 'The|DT',\n",
       " 'This|DT',\n",
       " 'also|RB',\n",
       " 'and|CC',\n",
       " 'an|DT',\n",
       " 'are|VBP',\n",
       " 'as|IN',\n",
       " 'at|IN',\n",
       " 'a|DT',\n",
       " 'be|VB',\n",
       " 'born|VBN',\n",
       " 'by|IN',\n",
       " 'can|MD',\n",
       " 'first|JJ',\n",
       " 'for|IN',\n",
       " 'from|IN',\n",
       " 'had|VBD',\n",
       " 'has|VBZ',\n",
       " 'he|PRP',\n",
       " 'his|PRP$',\n",
       " 'in|IN',\n",
       " 'is|VBZ',\n",
       " 'it|PRP',\n",
       " 'not|RB',\n",
       " 'of|IN',\n",
       " 'one|CD',\n",
       " 'on|IN',\n",
       " 'or|CC',\n",
       " 'people|NNS',\n",
       " 'that|IN',\n",
       " 'that|WDT',\n",
       " 'their|PRP$',\n",
       " 'they|PRP',\n",
       " 'the|DT',\n",
       " 'to|IN',\n",
       " 'to|TO',\n",
       " 'was|VBD',\n",
       " 'were|VBD',\n",
       " 'which|WDT',\n",
       " 'with|IN'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 20\n",
    "eval_count = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Grădina|NNP, Zoologică|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Ving|NNP, Rhames|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Karlovy|NNP, Vary|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Gharb|NNP-Chrarda|NNP-Beni|NNP, Hssen|NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.961128</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(waystations|NNS, shuku|NN-eki|NN)</td>\n",
       "      <td>20</td>\n",
       "      <td>-607.368914</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(supporters|NNS, of|IN, the|DT)</td>\n",
       "      <td>47</td>\n",
       "      <td>-3262.258611</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(episodes|NNS, and|CC)</td>\n",
       "      <td>64</td>\n",
       "      <td>-3262.274573</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(as|IN, old|JJ, as|IN)</td>\n",
       "      <td>48</td>\n",
       "      <td>-3262.394255</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(John|NNP, Kerry|NNP)</td>\n",
       "      <td>77</td>\n",
       "      <td>-3262.495991</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(is|VBZ, a|DT, bit|NN)</td>\n",
       "      <td>48</td>\n",
       "      <td>-3262.496741</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ngram  freq      poisson  len  \\\n",
       "0                      (Grădina|NNP, Zoologică|NNP)    20  -605.961128    2   \n",
       "1                            (Ving|NNP, Rhames|NNP)    20  -605.961128    2   \n",
       "2                           (Karlovy|NNP, Vary|NNP)    20  -605.961128    2   \n",
       "3       (Gharb|NNP-Chrarda|NNP-Beni|NNP, Hssen|NNP)    20  -605.961128    2   \n",
       "4                (waystations|NNS, shuku|NN-eki|NN)    20  -607.368914    2   \n",
       "...                                             ...   ...          ...  ...   \n",
       "149995              (supporters|NNS, of|IN, the|DT)    47 -3262.258611    3   \n",
       "149996                       (episodes|NNS, and|CC)    64 -3262.274573    2   \n",
       "149997                       (as|IN, old|JJ, as|IN)    48 -3262.394255    3   \n",
       "149998                        (John|NNP, Kerry|NNP)    77 -3262.495991    2   \n",
       "149999                       (is|VBZ, a|DT, bit|NN)    48 -3262.496741    3   \n",
       "\n",
       "        batch  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "149995      4  \n",
       "149996      3  \n",
       "149997     -1  \n",
       "149998      1  \n",
       "149999      1  \n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval = pd.read_pickle(datapath+'/Corpora/wiki/simple_20200601/Tagged/ngram_eval_clean.pkl')\n",
    "\n",
    "ngram_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = max(ngram_eval.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1     47463\n",
       " 2     33035\n",
       " 3     24270\n",
       " 4     16305\n",
       " 5     10058\n",
       " 6      7289\n",
       " 7      4847\n",
       " 8      2491\n",
       " 9      1465\n",
       " 10      942\n",
       "-2       814\n",
       " 11      457\n",
       "-1       186\n",
       " 12      171\n",
       " 13      118\n",
       " 14       65\n",
       " 15       24\n",
       "Name: batch, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Flatten down to a single number\n",
    "def cosim(x,y):\n",
    "    return cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 2 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 3 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 4 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 5 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 6 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 7 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 8 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 9 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 10 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 11 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 12 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 13 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 14 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 15 of 15\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Wall time: 2h 56min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "for bb in range(batch_count):\n",
    "    print('Processing batch {} of {}'.format(bb+1,batch_count))\n",
    "    \n",
    "    # Subset DataFrame\n",
    "    batch_dfs[bb] = ngram_eval[ngram_eval.batch == bb+1].reset_index(drop=True)\n",
    "    \n",
    "    # Initialise MWETokenizer\n",
    "    batch_token_mwe = MWETokenizer(list(batch_dfs[bb].ngram) , separator='+')\n",
    "    \n",
    "    # Build model\n",
    "    print('Building word2vec model')\n",
    "    sents_mwe = Simp_Sent_Seq(simp_t, tokenizer = batch_token_mwe)\n",
    "    \n",
    "    print(' Training model')\n",
    "    batch_model = Word2Vec(sents_mwe,\n",
    "                             min_count = 20,  # 20 matches R&E on EN Wiki\n",
    "                             size = 400,\n",
    "                             workers = 8,\n",
    "                             window = 5,\n",
    "                             sg = 0,         # CBOW\n",
    "                             sample = 10e-5, # Subsampling\n",
    "                             negative = 10\n",
    "                            )\n",
    "\n",
    "    # Save model\n",
    "    print(' Saving model')\n",
    "    batch_model.save(datapath+'/Models/1 w2v/Tagged/simp_tagged_clean_batch{}.model'.format(bb+1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
