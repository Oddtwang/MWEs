{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Google Drive\\\\University\\\\Dissertation\\\\Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/'+os.getlogin()+'/Google Drive/University/Dissertation'\n",
    "datapath = 'E:/Dissertation Data'\n",
    "\n",
    "os.chdir(path+'/Code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect lexical co-occurrence statistics on all words in\n",
    "the English Wikipedia, using the WikiExtractor tool2 to retrieve\n",
    "plain text from the April 2015 dump (ca. 2.8B words),\n",
    "and using simple regular expressions to segment sentences\n",
    "and words, and remove URLs and punctuation. We perform\n",
    "no POS tagging, lemmatisation, case normalisation,\n",
    "or removal of numbers or symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autism is a developmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior. \n",
      "\n",
      " Parents often notice signs during the first three years of their child's life. \n",
      "\n",
      " These signs often develop gradually, though some children with autism experience worsening in their communication and social skills after reaching developmental milestones at a normal pace. \n",
      "\n",
      " Autism is associated with a combination of genetic and environmental factors. \n",
      "\n",
      " Risk factors during pregnancy include certain infections, such as rubella, toxins including valproic acid, alcohol, cocaine, pesticides, lead, and air pollution, fetal growth restriction, and autoimmune diseases. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On English wiki, 10% sample\n",
    "\n",
    "sf = open(datapath+'/Corpora/wiki/enwiki_20200520/enwiki_20200520_10pc.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for lines in range(5):\n",
    "    print(sf.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w10p = PlaintextCorpusReader(datapath+'/Corpora/wiki/enwiki_20200520/','enwiki_20200520_10pc.txt',\n",
    "                            word_tokenizer = WhitespaceTokenizer()\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word and sentence generators\n",
    "\n",
    "from generators import sent_gen, word_gen, Sent_Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect word frequency information with the\n",
    "SRILM language modelling toolkit (Stolcke, 2002), counting\n",
    "n-grams (n <= 3), treating MWEs as contiguous bigrams\n",
    "and trigrams), and identify MWE candidates by computing\n",
    "the Poisson collocation measure (Quasthoff and Wolff,\n",
    "2002) for all bigrams and trigrams (ca. 23M n-grams).\n",
    "This method should be readily extensible to include longer\n",
    "n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate n-grams\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    TrigramAssocMeasures,\n",
    "    NgramAssocMeasures,\n",
    ")\n",
    "\n",
    "from nltk.metrics.spearman import (\n",
    "    spearman_correlation,\n",
    "    ranks_from_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then automatically score the million most strongly associated\n",
    "n-grams (i.e., roughly the top 5% of the Poisson-ranked\n",
    "list) for compositionality.\n",
    "\n",
    "Using word2vec (Mikolov et al., 2013) with the parameters\n",
    "found to be most effective by Baroni et al. (2014), we\n",
    "build a word embedding vector for every simplex word in\n",
    "the vocabulary (ca. 1M types), as well as for each MWE candidate.\n",
    "\n",
    "* Continuous bag of words model with 400-dimensional vectors, window size 5, subsampling with t = 10^-5, negative sampling with 10 samples. We build vectors only for tokens observed 20 times or more in the corpus.\n",
    "\n",
    "We then compute the cosine similarity of the vector\n",
    "representation for a MWE candidate with the vectors of its\n",
    "constituent words, and take the arithmetic mean. \n",
    "In scoring\n",
    "the compositionality of a candidate, we do not measure the\n",
    "cosine similarity of the MWE with any stop words it may\n",
    "contain, as stop words may be assumed to be semantically\n",
    "uninformative.\n",
    "* Stop words are taken here to be the 50 most frequent words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from corpus - 50 most frequent\n",
    "\n",
    "with open(datapath+'/Corpora/wiki/enwiki_20200520/10pc_stop.pkl', 'rb') as pfile:\n",
    "    stop = pickle.load(pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " 'A',\n",
       " 'He',\n",
       " 'In',\n",
       " 'It',\n",
       " 'New',\n",
       " 'The',\n",
       " 'a',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'been',\n",
       " 'but',\n",
       " 'by',\n",
       " 'first',\n",
       " 'for',\n",
       " 'from',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'her',\n",
       " 'his',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'or',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'this',\n",
       " 'to',\n",
       " 'two',\n",
       " 'was',\n",
       " 'were',\n",
       " 'which',\n",
       " 'who',\n",
       " 'with'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 20\n",
    "eval_count = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(of, the)</td>\n",
       "      <td>3.874652e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(References, External, links)</td>\n",
       "      <td>2.566994e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(External, links)</td>\n",
       "      <td>2.229096e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(in, the)</td>\n",
       "      <td>2.094387e+06</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 0, 0)</td>\n",
       "      <td>1.798530e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>(Work, started, on)</td>\n",
       "      <td>3.883693e+02</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>(of, these, techniques)</td>\n",
       "      <td>3.883679e+02</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>(Stadio, Flaminio)</td>\n",
       "      <td>3.883678e+02</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>(IRE, 3, C)</td>\n",
       "      <td>3.883678e+02</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>(The, Official, Encyclopedia)</td>\n",
       "      <td>3.883646e+02</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ngram       poisson  len  batch\n",
       "0                           (of, the)  3.874652e+06    2     -2\n",
       "1       (References, External, links)  2.566994e+06    3      1\n",
       "2                   (External, links)  2.229096e+06    2      2\n",
       "3                           (in, the)  2.094387e+06    2     -2\n",
       "4                           (0, 0, 0)  1.798530e+06    3     -2\n",
       "...                               ...           ...  ...    ...\n",
       "499995            (Work, started, on)  3.883693e+02    3      8\n",
       "499996        (of, these, techniques)  3.883679e+02    3      8\n",
       "499997             (Stadio, Flaminio)  3.883678e+02    2      1\n",
       "499998                    (IRE, 3, C)  3.883678e+02    3      1\n",
       "499999  (The, Official, Encyclopedia)  3.883646e+02    3      2\n",
       "\n",
       "[500000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval = pd.read_pickle(datapath+'/Corpora/wiki/enwiki_20200520/10pc_ngram_eval.pkl')\n",
    "\n",
    "ngram_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = max(ngram_eval.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1     94011\n",
       " 2     79538\n",
       " 3     69221\n",
       " 4     56154\n",
       " 5     48934\n",
       " 6     38288\n",
       " 7     30374\n",
       "-1     29076\n",
       " 8     22495\n",
       " 9     19272\n",
       " 10    11300\n",
       "-2      1337\n",
       "Name: batch, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Flatten down to a single number\n",
    "def cosim(x,y):\n",
    "    return cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 2 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 3 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 4 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 5 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 6 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 7 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 8 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 9 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Processing batch 10 of 10\n",
      "Building word2vec model\n",
      " Training model\n",
      " Saving model\n",
      "Wall time: 2d 11h 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "for bb in range(batch_count):\n",
    "    print('Processing batch {} of {}'.format(bb+1,batch_count))\n",
    "    \n",
    "    # Subset DataFrame\n",
    "    batch_dfs[bb] = ngram_eval[ngram_eval.batch == bb+1].reset_index(drop=True)\n",
    "    \n",
    "    # Initialise MWETokenizer\n",
    "    batch_token_mwe = MWETokenizer(list(batch_dfs[bb].ngram) , separator='+')\n",
    "    \n",
    "    # Build model\n",
    "    print('Building word2vec model')\n",
    "    sents_mwe = Sent_Seq(w10p, batch_token_mwe)\n",
    "    \n",
    "    print(' Training model')\n",
    "    batch_model = Word2Vec(sents_mwe,\n",
    "                             min_count = 20,  # 20 matches R&E on EN Wiki\n",
    "                             size = 400,\n",
    "                             workers = 8,\n",
    "                             window = 5,\n",
    "                             sg = 0,         # CBOW\n",
    "                             sample = 10e-5, # Subsampling\n",
    "                             negative = 10\n",
    "                            )\n",
    "\n",
    "    # Save model\n",
    "    print(' Saving model')\n",
    "    batch_model.save(datapath+'/Models/1 w2v/wiki10pc_batch{}.model'.format(bb+1))\n",
    "    # Reload looks like    new_model = gensim.models.Word2Vec.load('/tmp/mymodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
