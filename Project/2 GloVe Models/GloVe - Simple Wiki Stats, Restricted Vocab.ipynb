{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Google Drive\\\\University\\\\Dissertation\\\\Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'C:/Users/'+os.getlogin()+'/Google Drive/University/Dissertation'\n",
    "datapath = 'E:/Dissertation Data'\n",
    "#datapath = 'C:/Users/'+os.getlogin()+'/Dissertation Data'\n",
    "\n",
    "os.chdir(path+'/Code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect lexical co-occurrence statistics on all words in\n",
    "the English Wikipedia, using the WikiExtractor tool2 to retrieve\n",
    "plain text from the April 2015 dump (ca. 2.8B words),\n",
    "and using simple regular expressions to segment sentences\n",
    "and words, and remove URLs and punctuation. We perform\n",
    "no POS tagging, lemmatisation, case normalisation,\n",
    "or removal of numbers or symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April is the fourth month of the year, and comes between March and May. \n",
      "\n",
      " It is one of four months to have 30 days. \n",
      "\n",
      " April always begins on the same day of week as July, and additionally, January in leap years. \n",
      "\n",
      " April always ends on the same day of the week as December. \n",
      "\n",
      " April's flowers are the Sweet Pea and Daisy. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On Simple English wiki\n",
    "\n",
    "sf = open(datapath+'/Corpora/wiki/simple_20200601/simple_20200601_v2.txt', 'r', encoding='utf-8')\n",
    "\n",
    "for lines in range(5):\n",
    "    print(sf.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp = PlaintextCorpusReader(datapath+'/Corpora/wiki/simple_20200601/','simple_20200601_v2.txt',\n",
    "                            word_tokenizer = WhitespaceTokenizer()\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word and sentence generators\n",
    "\n",
    "from generators import sent_gen, word_gen, Sent_Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect word frequency information with the\n",
    "SRILM language modelling toolkit (Stolcke, 2002), counting\n",
    "n-grams (n <= 3), treating MWEs as contiguous bigrams\n",
    "and trigrams), and identify MWE candidates by computing\n",
    "the Poisson collocation measure (Quasthoff and Wolff,\n",
    "2002) for all bigrams and trigrams (ca. 23M n-grams).\n",
    "This method should be readily extensible to include longer\n",
    "n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate n-grams\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "\n",
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    TrigramAssocMeasures,\n",
    "    NgramAssocMeasures,\n",
    ")\n",
    "\n",
    "from nltk.metrics.spearman import (\n",
    "    spearman_correlation,\n",
    "    ranks_from_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load n-gram list produced earlier\n",
    "\n",
    "ngram_eval = pd.read_pickle(datapath+'/Corpora/wiki/simple_20200601/ngram_eval.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then automatically score the million most strongly associated\n",
    "n-grams (i.e., roughly the top 5% of the Poisson-ranked\n",
    "list) for compositionality.\n",
    "\n",
    "Using word2vec (Mikolov et al., 2013) with the parameters\n",
    "found to be most effective by Baroni et al. (2014), we\n",
    "build a word embedding vector for every simplex word in\n",
    "the vocabulary (ca. 1M types), as well as for each MWE candidate.\n",
    "\n",
    "* Continuous bag of words model with 400-dimensional vectors, window size 5, subsampling with t = 10^-5, negative sampling with 10 samples. We build vectors only for tokens observed 20 times or more in the corpus.\n",
    "\n",
    "We then compute the cosine similarity of the vector\n",
    "representation for a MWE candidate with the vectors of its\n",
    "constituent words, and take the arithmetic mean. \n",
    "In scoring\n",
    "the compositionality of a candidate, we do not measure the\n",
    "cosine similarity of the MWE with any stop words it may\n",
    "contain, as stop words may be assumed to be semantically\n",
    "uninformative.\n",
    "* Stop words are taken here to be the 50 most frequent words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from corpus - 50 most frequent\n",
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(word_gen(simp, sent_mark=''))\n",
    "\n",
    "stop = set( word for word, f in fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " 'A',\n",
       " 'American',\n",
       " 'He',\n",
       " 'In',\n",
       " 'It',\n",
       " 'References',\n",
       " 'The',\n",
       " 'This',\n",
       " 'a',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'born',\n",
       " 'by',\n",
       " 'can',\n",
       " 'first',\n",
       " 'for',\n",
       " 'from',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'his',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'not',\n",
       " 'of',\n",
       " 'on',\n",
       " 'one',\n",
       " 'or',\n",
       " 'people',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'they',\n",
       " 'to',\n",
       " 'was',\n",
       " 'were',\n",
       " 'which',\n",
       " 'with'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Ving, Rhames)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.625590</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Grădina, Zoologică)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.625590</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Gharb-Chrarda-Beni, Hssen)</td>\n",
       "      <td>20</td>\n",
       "      <td>-605.625590</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Karlovy, Vary)</td>\n",
       "      <td>20</td>\n",
       "      <td>-607.033377</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(waystations, shuku-eki)</td>\n",
       "      <td>20</td>\n",
       "      <td>-607.033377</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(first, country, to)</td>\n",
       "      <td>45</td>\n",
       "      <td>-3071.582792</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(from, A)</td>\n",
       "      <td>58</td>\n",
       "      <td>-3071.612088</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(1998, 8)</td>\n",
       "      <td>64</td>\n",
       "      <td>-3071.704678</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(had, tested, positive)</td>\n",
       "      <td>55</td>\n",
       "      <td>-3071.728206</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(23, 12)</td>\n",
       "      <td>64</td>\n",
       "      <td>-3071.731328</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ngram  freq      poisson  len  batch\n",
       "0                    (Ving, Rhames)    20  -605.625590    2      1\n",
       "1              (Grădina, Zoologică)    20  -605.625590    2      1\n",
       "2       (Gharb-Chrarda-Beni, Hssen)    20  -605.625590    2      1\n",
       "3                   (Karlovy, Vary)    20  -607.033377    2      1\n",
       "4          (waystations, shuku-eki)    20  -607.033377    2      1\n",
       "...                             ...   ...          ...  ...    ...\n",
       "149995         (first, country, to)    45 -3071.582792    3      2\n",
       "149996                    (from, A)    58 -3071.612088    2     -2\n",
       "149997                    (1998, 8)    64 -3071.704678    2      6\n",
       "149998      (had, tested, positive)    55 -3071.728206    3      4\n",
       "149999                     (23, 12)    64 -3071.731328    2      3\n",
       "\n",
       "[150000 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = max(ngram_eval.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1     45301\n",
       " 2     33312\n",
       " 3     24686\n",
       " 4     16668\n",
       " 5     10517\n",
       " 6      7502\n",
       " 7      4240\n",
       " 8      2702\n",
       " 9      1905\n",
       " 10     1051\n",
       "-2       832\n",
       " 11      450\n",
       "-1       300\n",
       " 12      247\n",
       " 13      123\n",
       " 14      107\n",
       " 15       57\n",
       "Name: batch, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Flatten down to a single number\n",
    "def cosim(x,y):\n",
    "    return cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwe_score(exp, model, stats_frame):\n",
    "    # Combined token for MWE\n",
    "    mwetoken = '+'.join(exp)\n",
    "\n",
    "    # Stopwords - 1 if component is a stopword, 0 if present, -1 if simplex word missing from vocab, -2 if MWE missing\n",
    "    sws = []\n",
    "    # Component vectors\n",
    "    cvs = []\n",
    "\n",
    "    #  Neighbours in original & MWE-aware space\n",
    "    oldn = []\n",
    "    newn = []\n",
    "\n",
    "    # List of individual word similarities (where present in the vocab)\n",
    "    css = []\n",
    "\n",
    "    # Empty array\n",
    "    earr = np.empty(1000)\n",
    "    earr[:] = np.nan\n",
    "\n",
    "    # Check that combined token exists in the vocab. This protects against inflation of n-gram counts caused by repeats\n",
    "    #  of the same token (e.g. in lists like https://simple.wikipedia.org/wiki/List_of_cities,_towns_and_villages_in_Fars_Province)\n",
    "    if mwetoken in model.dictionary:\n",
    "\n",
    "        mwv = model.word_vectors[model.dictionary[mwetoken]]\n",
    "\n",
    "        for w in exp:\n",
    "            if w in model.dictionary:\n",
    "                cvs.append(model.word_vectors[model.dictionary[w]])\n",
    "\n",
    "                #oldn.append(model.most_similar(w, number=5))\n",
    "\n",
    "                if w in stop:\n",
    "                    sws.append(1)\n",
    "                    css.append(np.nan)\n",
    "                else:\n",
    "                    sws.append(0)\n",
    "                    css.append(cosim(model.word_vectors[model.dictionary[w]], mwv ))\n",
    "\n",
    "            # If component is absent from vocab\n",
    "            else:\n",
    "                sws.append(-1)\n",
    "                cvs.append(earr)\n",
    "                css.append(np.nan)\n",
    "\n",
    "                #oldn.append([])\n",
    "\n",
    "        #  Mean cosim\n",
    "        if min(sws) >= 0:\n",
    "            cs = np.nanmean(css)\n",
    "        else:\n",
    "            cs = np.nan\n",
    "\n",
    "        #newn = model.most_similar(mwetoken, number=5)\n",
    "\n",
    "    # Combined token missing from vocab - mark with defaults\n",
    "    else:\n",
    "        sws = [-2]\n",
    "        mwv = np.empty(400)\n",
    "        mwv[:] = np.nan\n",
    "\n",
    "\n",
    "    # Append to stats df\n",
    "    return stats_frame.append({\n",
    "        'ngram'  : exp,\n",
    "        'stopwords' : sws,\n",
    "        'mwe_vector' : mwv,\n",
    "        'component_vectors' : cvs,\n",
    "        'component_cosims'  : css,\n",
    "        'cosine_sim'  : cs,\n",
    "        #'base_nearest': oldn,\n",
    "        #'mwe_nearest' : newn,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#batch_model = Glove.load(datapath+'/Models/2 GloVe/simple_batch{}.model'.format(bb+1))\n",
    "batch_model = Glove.load(datapath+'/Models/2 GloVe//simp_glove_vocab_batch2.model')\n",
    "\n",
    "batch_dict = batch_model.dictionary\n",
    "batch_vect = batch_model.word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ngram      (X-Men, Origins)\n",
       "freq                     22\n",
       "poisson             -832.45\n",
       "len                       2\n",
       "batch                     2\n",
       "Name: 2000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_eval.iloc[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_vect[batch_dict['X-Men+Origins']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_vect[batch_dict['X-Men']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_vect[batch_dict['Origins']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23099183684731814"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosim(batch_vect[batch_dict['X-Men']] , batch_vect[batch_dict['X-Men+Origins']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24027957618868892"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosim(batch_vect[batch_dict['Origins']] , batch_vect[batch_dict['X-Men+Origins']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hōan', 0.9959869363058576),\n",
       " ('Coo', 0.9955508478483007),\n",
       " ('1931–2014', 0.9954400864683283),\n",
       " ('Continuous', 0.9949582512584049)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_model.most_similar('X-Men+Origins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TheXverse.com', 0.7612342016656892),\n",
       " ('Astonishing', 0.5977794198210621),\n",
       " ('Apocalypse', 0.5654931846543487),\n",
       " ('Marvel.com', 0.49236101200406673)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_model.most_similar('X-Men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sfida', 0.6174009132645951),\n",
       " ('Kaitos', 0.5746709058795924),\n",
       " ('The+origins', 0.5296782733795666),\n",
       " ('left:30', 0.5097731461144712)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_model.most_similar('Origins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/45301: Ving+Rhames\n",
      " MWE 1000/45301: Real+Murcia\n",
      " MWE 2000/45301: Nguyen+dynasty's\n",
      " MWE 3000/45301: Cyndi+Lauper\n",
      " MWE 4000/45301: Dragon+Tattoo\n",
      " MWE 5000/45301: Southern+Ontario\n",
      " MWE 6000/45301: will+decide\n",
      " MWE 7000/45301: been+affected\n",
      " MWE 8000/45301: and+Gone\n",
      " MWE 9000/45301: being+chased\n",
      " MWE 10000/45301: decoration+presented\n",
      " MWE 11000/45301: Swiss+Federal+Councillor\n",
      " MWE 12000/45301: was+welcomed\n",
      " MWE 13000/45301: at+Duke\n",
      " MWE 14000/45301: more+copies\n",
      " MWE 15000/45301: Without+Borders\n",
      " MWE 16000/45301: insurance+companies\n",
      " MWE 17000/45301: from+disease\n",
      " MWE 18000/45301: the+Willis+Tower\n",
      " MWE 19000/45301: mounted+in\n",
      " MWE 20000/45301: his+twin+brother\n",
      " MWE 21000/45301: his+queen\n",
      " MWE 22000/45301: Nazi+Germany+in\n",
      " MWE 23000/45301: Manchester+in\n",
      " MWE 24000/45301: now+used+by\n",
      " MWE 25000/45301: as+the+Kingdom\n",
      " MWE 26000/45301: Consadole+Sapporo+Football\n",
      " MWE 27000/45301: was+eight+years\n",
      " MWE 28000/45301: to+his+work\n",
      " MWE 29000/45301: is+partially\n",
      " MWE 30000/45301: a+multinational\n",
      " MWE 31000/45301: and+martial+artist\n",
      " MWE 32000/45301: Aires+on\n",
      " MWE 33000/45301: the+Centre-Val\n",
      " MWE 34000/45301: Republican+in\n",
      " MWE 35000/45301: and+Kurt\n",
      " MWE 36000/45301: songwriter+and+actor\n",
      " MWE 37000/45301: stop+working\n",
      " MWE 38000/45301: Paul+Williams\n",
      " MWE 39000/45301: at+dawn\n",
      " MWE 40000/45301: to+the+Medal\n",
      " MWE 41000/45301: statistics+1991\n",
      " MWE 42000/45301: only+used+for\n",
      " MWE 43000/45301: Deportivo+La\n",
      " MWE 44000/45301: also+need\n",
      " MWE 45000/45301: his+school\n",
      "Processing batch 2 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/33312: IK+Sävehof\n",
      " MWE 1000/33312: Cabinet+Minister\n",
      " MWE 2000/33312: in+Buckinghamshire\n",
      " MWE 3000/33312: 8+minutes\n",
      " MWE 4000/33312: Maxwell's+equations\n",
      " MWE 5000/33312: Senate+confirmed\n",
      " MWE 6000/33312: years+her\n",
      " MWE 7000/33312: The+Kennedy/Marshall\n",
      " MWE 8000/33312: by+Ralph+Fletcher\n",
      " MWE 9000/33312: in+Hanoi\n",
      " MWE 10000/33312: Press+2004\n",
      " MWE 11000/33312: summit+was\n",
      " MWE 12000/33312: While+they+were\n",
      " MWE 13000/33312: Western+North+America\n",
      " MWE 14000/33312: fiction+series\n",
      " MWE 15000/33312: all+people+who\n",
      " MWE 16000/33312: adults+have\n",
      " MWE 17000/33312: than+him\n",
      " MWE 18000/33312: Women+were\n",
      " MWE 19000/33312: In+December+2018\n",
      " MWE 20000/33312: agreeing+to\n",
      " MWE 21000/33312: River+which+is\n",
      " MWE 22000/33312: Alpha+Ethniki\n",
      " MWE 23000/33312: in+the+WHL\n",
      " MWE 24000/33312: of+the+attacks\n",
      " MWE 25000/33312: A+public\n",
      " MWE 26000/33312: Singles+Year+Title\n",
      " MWE 27000/33312: As+time\n",
      " MWE 28000/33312: of+Units\n",
      " MWE 29000/33312: be+regarded+as\n",
      " MWE 30000/33312: in+there\n",
      " MWE 31000/33312: usually+considered\n",
      " MWE 32000/33312: two+years+before\n",
      " MWE 33000/33312: after+they+have\n",
      "Processing batch 3 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/24686: Goh+Chok+Tong\n",
      " MWE 1000/24686: competition+held\n",
      " MWE 2000/24686: music+it\n",
      " MWE 3000/24686: to+flood\n",
      " MWE 4000/24686: to+world\n",
      " MWE 5000/24686: District+Tirana\n",
      " MWE 6000/24686: Soul+Train\n",
      " MWE 7000/24686: job+satisfaction\n",
      " MWE 8000/24686: cavalry+and\n",
      " MWE 9000/24686: USA+for\n",
      " MWE 10000/24686: game+company\n",
      " MWE 11000/24686: its+unique\n",
      " MWE 12000/24686: infantry+and\n",
      " MWE 13000/24686: total+of+about\n",
      " MWE 14000/24686: the+Whig+Party\n",
      " MWE 15000/24686: content+in\n",
      " MWE 16000/24686: Calhoun+County\n",
      " MWE 17000/24686: Calvin+and\n",
      " MWE 18000/24686: soldier+d\n",
      " MWE 19000/24686: record+deal+with\n",
      " MWE 20000/24686: Medical+College\n",
      " MWE 21000/24686: Tierra+del+Fuego\n",
      " MWE 22000/24686: videos+on\n",
      " MWE 23000/24686: One+reason\n",
      " MWE 24000/24686: played+for+Argentina\n",
      "Processing batch 4 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/16668: 8:30+9:00+9:30\n",
      " MWE 1000/16668: actress+1942\n",
      " MWE 2000/16668: her+mouth\n",
      " MWE 3000/16668: with+17\n",
      " MWE 4000/16668: involved+in+many\n",
      " MWE 5000/16668: founded+by+a\n",
      " MWE 6000/16668: in+32\n",
      " MWE 7000/16668: 1999+16\n",
      " MWE 8000/16668: what+was+called\n",
      " MWE 9000/16668: years+The\n",
      " MWE 10000/16668: are+old\n",
      " MWE 11000/16668: piece+was\n",
      " MWE 12000/16668: Richard+E\n",
      " MWE 13000/16668: Territory+is\n",
      " MWE 14000/16668: Lonely+Hearts\n",
      " MWE 15000/16668: patients+who\n",
      " MWE 16000/16668: providing+the\n",
      "Processing batch 5 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/10517: 9:00+9:30+10:00\n",
      " MWE 1000/10517: High+King\n",
      " MWE 2000/10517: January+7+2013\n",
      " MWE 3000/10517: an+address\n",
      " MWE 4000/10517: 14+episodes\n",
      " MWE 5000/10517: complexity+of\n",
      " MWE 6000/10517: spent+three\n",
      " MWE 7000/10517: was+renovated\n",
      " MWE 8000/10517: Portuguese+football+player\n",
      " MWE 9000/10517: flowering+plants+in\n",
      " MWE 10000/10517: a+1988\n",
      "Processing batch 6 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/7502: d’Art+Moderne+de\n",
      " MWE 1000/7502: of+Germany+under\n",
      " MWE 2000/7502: in+English+for\n",
      " MWE 3000/7502: vocals+bass\n",
      " MWE 4000/7502: not+suitable+for\n",
      " MWE 5000/7502: Governor+of+Oregon\n",
      " MWE 6000/7502: organization+is\n",
      " MWE 7000/7502: the+Prairie\n",
      "Processing batch 7 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/4240: by+movie\n",
      " MWE 200/4240: South+Carolina+from\n",
      " MWE 400/4240: system+it\n",
      " MWE 600/4240: over+parts+of\n",
      " MWE 800/4240: They+made+it\n",
      " MWE 1000/4240: Tennessee+and+in\n",
      " MWE 1200/4240: can+be+performed\n",
      " MWE 1400/4240: a+Good\n",
      " MWE 1600/4240: Conference+and\n",
      " MWE 1800/4240: a+Russian+politician\n",
      " MWE 2000/4240: majority+government\n",
      " MWE 2200/4240: various+places\n",
      " MWE 2400/4240: companies+have\n",
      " MWE 2600/4240: born+in+Manhattan\n",
      " MWE 2800/4240: jump+and\n",
      " MWE 3000/4240: Progressive+Conservative+Party\n",
      " MWE 3200/4240: river+are+Left\n",
      " MWE 3400/4240: Silk+Road\n",
      " MWE 3600/4240: to+a+son\n",
      " MWE 3800/4240: east+bank+of\n",
      " MWE 4000/4240: a+Triple\n",
      " MWE 4200/4240: not+die\n",
      "Processing batch 8 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/2702: 6+31\n",
      " MWE 200/2702: was+having+a\n",
      " MWE 400/2702: the+Queen+Elizabeth\n",
      " MWE 600/2702: London+City\n",
      " MWE 800/2702: away+from+Earth\n",
      " MWE 1000/2702: performed+in+a\n",
      " MWE 1200/2702: were+inducted+into\n",
      " MWE 1400/2702: Dance+before\n",
      " MWE 1600/2702: the+Court+ruled\n",
      " MWE 1800/2702: like+being\n",
      " MWE 2000/2702: the+supervision+of\n",
      " MWE 2200/2702: the+diameter\n",
      " MWE 2400/2702: was+eight\n",
      " MWE 2600/2702: again+this\n",
      "Processing batch 9 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/1905: far+more+than\n",
      " MWE 200/1905: in+November+and\n",
      " MWE 400/1905: 7+Ret+5\n",
      " MWE 600/1905: 2019+A\n",
      " MWE 800/1905: August+2019+at\n",
      " MWE 1000/1905: three+most\n",
      " MWE 1200/1905: of+the+normal\n",
      " MWE 1400/1905: after+having+been\n",
      " MWE 1600/1905: on+15+February\n",
      " MWE 1800/1905: on+13+September\n",
      "Processing batch 10 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/1051: like+New\n",
      " MWE 200/1051: 10+1+3\n",
      " MWE 400/1051: official+name+after\n",
      " MWE 600/1051: on+the+lake\n",
      " MWE 800/1051: American+company\n",
      " MWE 1000/1051: those+who+were\n",
      "Processing batch 11 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/450: 0+1986+0\n",
      " MWE 200/450: that+they+use\n",
      " MWE 400/450: of+singles\n",
      "Processing batch 12 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/247: in+Economics+in\n",
      " MWE 200/247: France+during\n",
      "Processing batch 13 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/123: of+India+on\n",
      "Processing batch 14 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/107: 0+54+0\n",
      "Processing batch 15 of 15\n",
      "Loading GloVe model\n",
      "Gathering MWE stats\n",
      " MWE 0/57: 0+2009+10\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "for bb in range(batch_count):\n",
    "    print('Processing batch {} of {}'.format(bb+1,batch_count))\n",
    "    \n",
    "    # Subset DataFrame\n",
    "    batch_dfs[bb] = ngram_eval[ngram_eval.batch == bb+1].reset_index(drop=True)\n",
    "    \n",
    "    # Initialise MWETokenizer\n",
    "    batch_token_mwe = MWETokenizer(list(batch_dfs[bb].ngram) , separator='+')\n",
    "    \n",
    "    # Load model\n",
    "    print('Loading GloVe model')\n",
    "    \n",
    "    batch_model = Glove.load(datapath+'/Models/2 GloVe//simp_glove_vocab_batch{}.model'.format(bb+1))\n",
    "    \n",
    "    print('Gathering MWE stats')\n",
    "     # For each MWE, evaluate stats. Record vectors (in case we want to calculate different metrics later).\n",
    "    statsf = pd.DataFrame(columns=['ngram', 'stopwords', 'mwe_vector', 'component_vectors', 'component_cosims', \n",
    "                                   'cosine_sim'])  #, 'base_nearest', 'mwe_nearest'\n",
    "\n",
    "    batch_len = len(batch_dfs[bb].ngram)\n",
    "    if batch_len >= 5000: \n",
    "        printer = 1000\n",
    "    else:\n",
    "        printer = 200\n",
    "        \n",
    "    _i = 0\n",
    "    \n",
    "    for exp in batch_dfs[bb].ngram:\n",
    "        if _i % printer == 0:\n",
    "            print(' MWE '+str(_i)+'/'+str(batch_len)+': '+'+'.join(exp))\n",
    "        _i += 1\n",
    "            \n",
    "        statsf = mwe_score(exp,batch_model,statsf)\n",
    "\n",
    "      #Join back onto DataFrame\n",
    "    batch_dfs[bb] = batch_dfs[bb].merge(statsf, on='ngram')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed\n",
    "    #print('Gathering MWE stats')\n",
    "    # For each MWE, evaluate stats. Record vectors (in case we want to calculate different metrics later).\n",
    "    #statsf = pd.DataFrame(columns=['ngram', 'stopwords', 'mwe_vector', 'component_vectors', 'component_cosims', \n",
    "    #                               'cosine_sim', 'base_nearest', 'mwe_nearest'])\n",
    "\n",
    "    #for exp in batch_dfs[bb].ngram:\n",
    "    #    statsf = mwe_score(exp,batch_model,statsf)\n",
    "\n",
    "    #  Join back onto DataFrame\n",
    "    #batch_dfs[bb] = batch_dfs[bb].merge(statsf, on='ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes, sort by compositionality metric, export\n",
    "\n",
    "# Also want the default batches with batch no < 0\n",
    "all_batches = ngram_eval[ngram_eval.batch < 0]\n",
    "\n",
    "for d in range(batch_count):\n",
    "    all_batches = all_batches.append(batch_dfs[d])\n",
    "    \n",
    "all_batches = all_batches.sort_values('cosine_sim')\n",
    "all_batches = all_batches.reset_index(drop=True)\n",
    "\n",
    "all_batches.to_csv(datapath+'/Models/2 GloVe/Results/simple_vocab_output_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "      <th>poisson</th>\n",
       "      <th>len</th>\n",
       "      <th>batch</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>mwe_vector</th>\n",
       "      <th>component_vectors</th>\n",
       "      <th>component_cosims</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, highest-rated)</td>\n",
       "      <td>22</td>\n",
       "      <td>-1025.646943</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[-1.9905872957151742, -1.9976834386702171, -1....</td>\n",
       "      <td>[[0.006619947186335439, 0.14868128607595, -0.1...</td>\n",
       "      <td>[nan, -0.9999907006105433]</td>\n",
       "      <td>-0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(was, facelifted, in)</td>\n",
       "      <td>24</td>\n",
       "      <td>-1540.866708</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>[-5.886014101390828, -5.921196131138483, 5.922...</td>\n",
       "      <td>[[0.16271233754597866, -0.04303778304083927, 0...</td>\n",
       "      <td>[nan, -0.9999871461769124, nan]</td>\n",
       "      <td>-0.999987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(the, station's)</td>\n",
       "      <td>44</td>\n",
       "      <td>-2069.249614</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[1.8662915200814416, 1.8526063958224677, 1.870...</td>\n",
       "      <td>[[0.006619947186335439, 0.14868128607595, -0.1...</td>\n",
       "      <td>[nan, -0.9999648035199541]</td>\n",
       "      <td>-0.999965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(in, the, shade)</td>\n",
       "      <td>21</td>\n",
       "      <td>-1431.207176</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>[-1.8775673300712086, 1.8953319133441393, -1.8...</td>\n",
       "      <td>[[0.10856749640439656, -0.32831220974975284, 0...</td>\n",
       "      <td>[nan, nan, -0.9999537329974788]</td>\n",
       "      <td>-0.999954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(The, Comunità)</td>\n",
       "      <td>36</td>\n",
       "      <td>-1639.428758</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[1.426591690916393, 1.4026078498029213, 1.4056...</td>\n",
       "      <td>[[-0.00473311056862602, 0.2508571552947894, -0...</td>\n",
       "      <td>[nan, -0.9999526702180579]</td>\n",
       "      <td>-0.999953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>(in, its, 1)</td>\n",
       "      <td>43</td>\n",
       "      <td>-3066.527125</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>(the, disease, at)</td>\n",
       "      <td>44</td>\n",
       "      <td>-3070.154899</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>(America, during, the)</td>\n",
       "      <td>45</td>\n",
       "      <td>-3071.046482</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>(4, 3, 4)</td>\n",
       "      <td>46</td>\n",
       "      <td>-3071.054761</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>(from, A)</td>\n",
       "      <td>58</td>\n",
       "      <td>-3071.612088</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ngram  freq      poisson  len  batch  stopwords  \\\n",
       "0         (the, highest-rated)    22 -1025.646943    2      1     [1, 0]   \n",
       "1        (was, facelifted, in)    24 -1540.866708    3      5  [1, 0, 1]   \n",
       "2             (the, station's)    44 -2069.249614    2      1     [1, 0]   \n",
       "3             (in, the, shade)    21 -1431.207176    3      2  [1, 1, 0]   \n",
       "4              (The, Comunità)    36 -1639.428758    2      1     [1, 0]   \n",
       "...                        ...   ...          ...  ...    ...        ...   \n",
       "149995            (in, its, 1)    43 -3066.527125    3     -1        NaN   \n",
       "149996      (the, disease, at)    44 -3070.154899    3     -1        NaN   \n",
       "149997  (America, during, the)    45 -3071.046482    3     -1        NaN   \n",
       "149998               (4, 3, 4)    46 -3071.054761    3     -2        NaN   \n",
       "149999               (from, A)    58 -3071.612088    2     -2        NaN   \n",
       "\n",
       "                                               mwe_vector  \\\n",
       "0       [-1.9905872957151742, -1.9976834386702171, -1....   \n",
       "1       [-5.886014101390828, -5.921196131138483, 5.922...   \n",
       "2       [1.8662915200814416, 1.8526063958224677, 1.870...   \n",
       "3       [-1.8775673300712086, 1.8953319133441393, -1.8...   \n",
       "4       [1.426591690916393, 1.4026078498029213, 1.4056...   \n",
       "...                                                   ...   \n",
       "149995                                                NaN   \n",
       "149996                                                NaN   \n",
       "149997                                                NaN   \n",
       "149998                                                NaN   \n",
       "149999                                                NaN   \n",
       "\n",
       "                                        component_vectors  \\\n",
       "0       [[0.006619947186335439, 0.14868128607595, -0.1...   \n",
       "1       [[0.16271233754597866, -0.04303778304083927, 0...   \n",
       "2       [[0.006619947186335439, 0.14868128607595, -0.1...   \n",
       "3       [[0.10856749640439656, -0.32831220974975284, 0...   \n",
       "4       [[-0.00473311056862602, 0.2508571552947894, -0...   \n",
       "...                                                   ...   \n",
       "149995                                                NaN   \n",
       "149996                                                NaN   \n",
       "149997                                                NaN   \n",
       "149998                                                NaN   \n",
       "149999                                                NaN   \n",
       "\n",
       "                       component_cosims  cosine_sim  \n",
       "0            [nan, -0.9999907006105433]   -0.999991  \n",
       "1       [nan, -0.9999871461769124, nan]   -0.999987  \n",
       "2            [nan, -0.9999648035199541]   -0.999965  \n",
       "3       [nan, nan, -0.9999537329974788]   -0.999954  \n",
       "4            [nan, -0.9999526702180579]   -0.999953  \n",
       "...                                 ...         ...  \n",
       "149995                              NaN         NaN  \n",
       "149996                              NaN         NaN  \n",
       "149997                              NaN         NaN  \n",
       "149998                              NaN         NaN  \n",
       "149999                              NaN         NaN  \n",
       "\n",
       "[150000 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches_light = all_batches.drop(columns=['mwe_vector', 'component_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches_light.to_csv(datapath+'/Models/2 GloVe/Results/simple_vocab_light_001.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
